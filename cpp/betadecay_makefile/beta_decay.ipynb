{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "casual-third",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Beta Decay Regression\n",
    "\n",
    "This notebook is a simplified version of the beta decay regression notebook in `examples/Beta-Decay.ipy`. Please refer to the original notebook for detailed discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set use_cuda=True to use an available GPU (for reference)\n",
    "# This doesn't really help for this example though since making plots\n",
    "# in each training iteration requires copying tensors to the CPU.\n",
    "use_cuda=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set use_exact_model=True to use a model parameterized\n",
    "# exactly like the analytic solution\n",
    "use_exact_model=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytic Solution & RHS\n",
    "\n",
    "# initial conditions at \"time\" 0\n",
    "y0_t0 = 1\n",
    "y1_t0 = 0\n",
    "\n",
    "# exponential decay constant\n",
    "exp_A = 1\n",
    "\n",
    "# this is the analytic solution of the system as a function of x\n",
    "def sol(_x):\n",
    "    # ya & yb are tensors that let us express the analytic\n",
    "    # solution of the system in matrix form\n",
    "    ya = torch.ones(1,2)\n",
    "    ya[0][0] =  y0_t0\n",
    "    ya[0][1] = -y0_t0\n",
    "\n",
    "    yb = torch.ones(1,2)\n",
    "    yb[0][0] = 0\n",
    "    yb[0][1] = y0_t0 + y1_t0\n",
    "    \n",
    "    return torch.exp(-exp_A * _x) * ya + yb\n",
    "\n",
    "# this is the analytic derivative of the system w.r.t. x\n",
    "def rhs(_y):\n",
    "    yb = torch.ones(1,2)\n",
    "    yb[0][0] = 0\n",
    "    yb[0][1] = y0_t0 + y1_t0\n",
    "    \n",
    "    return -exp_A * (_y - yb)\n",
    "\n",
    "# if we're using cuda, then put the tensors\n",
    "# in our workspace on the GPU.\n",
    "if use_cuda:\n",
    "    x = x.cuda()\n",
    "    x_test = x_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-patio",
   "metadata": {},
   "source": [
    "## Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate NumSamples for training\n",
    "NumSamples = 128\n",
    "NumTest = NumSamples//2\n",
    "\n",
    "# let's look at a range in x from [0, 10]\n",
    "xmin = 0\n",
    "xmax = 10.0\n",
    "\n",
    "# the range of the solution values is [0,1]\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "x = torch.unsqueeze(torch.linspace(xmin, xmax, NumSamples, requires_grad=True), dim=1)\n",
    "x_test = torch.unsqueeze(torch.rand(NumTest, requires_grad=True), dim=1) * (xmax-xmin) + xmin\n",
    "\n",
    "# get the analytic solution as a function of x\n",
    "y = sol(x)\n",
    "\n",
    "# get the analytic right-hand-side as a function of y(x)\n",
    "# f(x) = dy(x)/dx\n",
    "dydx = rhs(y)\n",
    "\n",
    "# get the analytic solution at the test points x_test\n",
    "y_test = sol(x_test)\n",
    "    \n",
    "# we will want to propagate gradients through y, dydx, and x\n",
    "# so make them PyTorch Variables\n",
    "x = Variable(x, requires_grad=True)\n",
    "y = Variable(y, requires_grad=True)\n",
    "dydx = Variable(dydx, requires_grad=True)\n",
    "\n",
    "# we will need to evaluate gradients w.r.t. x multiple\n",
    "# times so tell PyTorch to save the gradient variable in x.\n",
    "x.retain_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-commonwealth",
   "metadata": {},
   "source": [
    "## Setting up the Models, Optimizers & Loss Function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenNet(nn.Module):\n",
    "    def __init__(self, n_independent, n_dependent,\n",
    "                 n_hidden, hidden_depth, activation):\n",
    "        super(HiddenNet, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.input_layer = nn.Linear(n_independent, n_hidden)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(hidden_depth):\n",
    "            self.hidden_layers.append(nn.Linear(n_hidden, n_hidden))\n",
    "        self.output_layer = nn.Linear(n_hidden, n_dependent)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for h in self.hidden_layers:\n",
    "            x = self.activation(h(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.y0_0 = torch.nn.Parameter(torch.tensor(1.0))\n",
    "        self.y1_0 = torch.nn.Parameter(torch.tensor(1.0))\n",
    "        self.a = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ya & yb are tensors that let us express the analytic\n",
    "        # solution of the system in matrix form\n",
    "        ya = torch.ones(1,2)\n",
    "        ya[0][0] =  self.y0_0\n",
    "        ya[0][1] = -self.y0_0\n",
    "\n",
    "        yb = torch.ones(1,2)\n",
    "        yb[0][0] = 0\n",
    "        yb[0][1] = self.y0_0 + self.y1_0\n",
    "        \n",
    "        return torch.exp(-self.a * x) * ya + yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "if use_exact_model:\n",
    "    net = ExactModel()\n",
    "else:\n",
    "    net = HiddenNet(n_independent=1, n_dependent=2,\n",
    "                    n_hidden=2, hidden_depth=0, activation=F.celu)\n",
    "\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "optimizer_sgd = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer_adam = torch.optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-posting",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_error(NumEpochs):\n",
    "    for t in range(NumEpochs):\n",
    "        # calculate prediction given the current net state\n",
    "        prediction = net(x)\n",
    "        \n",
    "        # calculate error between prediction and analytic truth y\n",
    "        loss0 = torch.sqrt(loss_func(prediction, y))\n",
    "\n",
    "        # calculate gradients d(prediction)/d(x) for each component\n",
    "\n",
    "        # first, zero out the existing gradients to avoid\n",
    "        # accumulating gradients on top of existing gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        if x.grad is not None:\n",
    "            x.grad.data.zero_()\n",
    "\n",
    "        # now get the gradients dp0/dx\n",
    "        prediction[:,0].backward(torch.ones_like(prediction[:,0]), retain_graph=True)\n",
    "        # clone the x gradient to save a copy of it as dp0/dx\n",
    "        dp0dx = x.grad.clone()\n",
    "        # clear the x gradient for the loss gradient below\n",
    "        x.grad.data.zero_()\n",
    "        \n",
    "        # get gradient dp1/dx\n",
    "        prediction[:,1].backward(torch.ones_like(prediction[:,1]), retain_graph=True)\n",
    "        # clone the x gradient to save a copy of it as dp1/dx\n",
    "        dp1dx = x.grad.clone()\n",
    "        # clear the x gradient for the loss gradient below\n",
    "        x.grad.data.zero_()\n",
    "        \n",
    "        dpdx = torch.ones_like(prediction)\n",
    "        dpdx[:,0] = torch.flatten(dp0dx)\n",
    "        dpdx[:,1] = torch.flatten(dp1dx)\n",
    "        \n",
    "        # evaluate the analytic right-hand-side function at the prediction value\n",
    "        prhs = rhs(prediction)\n",
    "\n",
    "        # define the error of the prediction derivative using the analytic derivative\n",
    "        loss1 = torch.sqrt(loss_func(dpdx, dydx))\n",
    "        \n",
    "        # the following doesn't work well :/\n",
    "        #loss1 = torch.sqrt(loss_func(dpdx, rhs(prediction)))\n",
    "\n",
    "        # total error combines the error of the prediction (loss0) with \n",
    "        # the error of the prediction derivative (loss1)\n",
    "        loss = loss0 + loss1\n",
    "\n",
    "        # use the Adam optimizer\n",
    "        optimizer = optimizer_adam\n",
    "\n",
    "        # clear gradients for the next training iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute backpropagation gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # apply gradients to update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get error with testing samples\n",
    "        # first, turn off training\n",
    "        net.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction_test = net(x_test)\n",
    "            test_loss = torch.sqrt(loss_func(prediction_test, y_test)).cpu().data.numpy()\n",
    "\n",
    "        # turn back on training\n",
    "        net.train()\n",
    "        \n",
    "        # Print epoch/error notifications\n",
    "        if t%100 == 0:\n",
    "            print(\"epoch \", t, \" with error: \", loss.item())\n",
    "    \n",
    "        # Stop early if our errors are plateauing\n",
    "        if t > 1000:\n",
    "            # do a quadratic polynomial fit and see if we will\n",
    "            # need more than NumEpochs for the error e to vanish:\n",
    "            # e / (d(e)/d(epoch)) > NumEpochs ?\n",
    "            # if so, then break out of the training loop ...\n",
    "            xfit = epochs[-4:]\n",
    "            efit = losses[-4:]\n",
    "            coef = np.polyfit(xfit, efit, 2)\n",
    "            \n",
    "            if coef[2]/coef[1] > NumEpochs:\n",
    "                break\n",
    "    \n",
    "    print(\"final testing error: \", test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-uniform",
   "metadata": {},
   "source": [
    "# Converting to Torch Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_module = torch.jit.script(net)\n",
    "script_module.save(\"betadecay_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99198a20-deb9-4171-a8d2-5f59f29e351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert x_test to numpy array and write to file\n",
    "xnp = x_test.cpu().data.numpy()\n",
    "file = open(\"test_data.txt\", \"w\")\n",
    "\n",
    "file.write(str(len(xnp)) + \"\\n\")\n",
    "for row in xnp:\n",
    "    np.savetxt(file, row)\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write y_test to file\n",
    "ynp = y_test.cpu().data.numpy()\n",
    "\n",
    "file = open(\"test_output.txt\", \"w\")\n",
    "\n",
    "file.write(str(len(ynp)) + \"\\n\")\n",
    "for row in ynp:\n",
    "    file.write(\"{0:.16e}   {1:.16e}\\n\".format(row[0], row[1]))\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1fb86a-b225-4961-a7c1-5f6338fb4b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a99f3-f3f9-48bd-be07-987a93ff8287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
