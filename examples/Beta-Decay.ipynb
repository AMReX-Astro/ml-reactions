{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta Decay Regression with PyTorch\n",
    "\n",
    "This notebook uses PyTorch to do ML regression of the solution to the ODE system describing beta decay as a simple proxy for more complicated reaction networks.\n",
    "\n",
    "The two-component system is:\n",
    "\n",
    "$$\\dot{X}_0 (t) = - A \\cdot X_0 (t)$$\n",
    "$$\\dot{X}_1 (t) = + A \\cdot X_0 (t)$$\n",
    "\n",
    "This system has an analytic solution we can compare with the model predictions:\n",
    "\n",
    "$$X_0 (t) = X_0 (0) \\cdot e^{-A \\cdot t}$$\n",
    "$$X_1 (t) = X_1 (0) + X_0 (0) \\cdot \\left(1 - e^{-A \\cdot t}\\right)$$\n",
    "\n",
    "For this example, the exponential decay constant $A$ and the initial conditions are set to:\n",
    "\n",
    "$$A = 1$$\n",
    "$$X_0 (0) = 1$$\n",
    "$$X_1 (0) = 0$$\n",
    "\n",
    "The model input is the time $t$ and the outputs are $X_0 (t)$ and $X_1 (t)$.\n",
    "\n",
    "(Although the below is written as $y(x)$ instead of $x(t)$ unfortunately, maybe I'll eventually change it...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "%matplotlib agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Options\n",
    "\n",
    "`use_cuda = [True] or [False]`\n",
    "\n",
    "This notebook is set up to allow training on a GPU or CPU.\n",
    "\n",
    "Because we are generating plot data within the training loop, GPU training isn't particularly efficient. But it is a reference to the PyTorch API.\n",
    "\n",
    "`use_exact_model = [True] or [False]`\n",
    "\n",
    "There are two models for training here:\n",
    "\n",
    "1. A feed-forward multi-layer neural network\n",
    "\n",
    "Number of independent variables (inputs), dependent variables (outputs), number of hidden layers, number of nodes per hidden layer, and activation function are customizable.\n",
    "\n",
    "2. The analytic solution to the ODE system\n",
    "\n",
    "The analytic solution to the ODE system is parameterized by the initial conditions and the exponential decay constant. Setting `use_exact_model=True` demonstrates that the backpropagation training scheme can use the parameterized ODE solution to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set use_cuda=True to use an available GPU (for reference)\n",
    "# This doesn't really help for this example though since making plots\n",
    "# in each training iteration requires copying tensors to the CPU.\n",
    "## NOTE: I've commented this out because a global variable in the notebook like this makes it so PyTorch cannot compile the model to TorchScript.\n",
    "##       There are workarounds, like making the model construction & training a function, which we would use for production code.\n",
    "use_cuda=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set use_exact_model=True to use a model parameterized\n",
    "# exactly like the analytic solution\n",
    "use_exact_model=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Data\n",
    "\n",
    "### Number of Samples & timescale for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate NumSamples for training\n",
    "NumSamples = 128\n",
    "\n",
    "# let's look at a range in x from [0, 10]\n",
    "xmin = 0\n",
    "xmax = 10.0\n",
    "\n",
    "# the range of the solution values is [0,1]\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "\n",
    "x = torch.unsqueeze(torch.linspace(xmin, xmax, NumSamples, requires_grad=True), dim=1)\n",
    "x_test = torch.unsqueeze(torch.rand(NumSamples, requires_grad=True), dim=1) * (xmax-xmin) + xmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic Solution & RHS Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial conditions at \"time\" 0\n",
    "y0_t0 = 1\n",
    "y1_t0 = 0\n",
    "\n",
    "# exponential decay constant\n",
    "exp_A = 1\n",
    "\n",
    "# ya & yb are tensors that let us express the analytic\n",
    "# solution of the system in matrix form\n",
    "ya = torch.ones(1,2)\n",
    "ya[0][0] =  y0_t0\n",
    "ya[0][1] = -y0_t0\n",
    "\n",
    "yb = torch.ones(1,2)\n",
    "yb[0][0] = 0\n",
    "yb[0][1] = y0_t0 + y1_t0\n",
    "\n",
    "# this is the analytic solution of the system as a function of x\n",
    "def sol(_x):\n",
    "    return torch.exp(-exp_A * _x) * ya + yb\n",
    "\n",
    "# this is the analytic derivative of the system w.r.t. x\n",
    "def rhs(_y):\n",
    "    return -exp_A * (_y - yb)\n",
    "\n",
    "# if we're using cuda, then put the tensors\n",
    "# in our workspace on the GPU.\n",
    "#if use_cuda:\n",
    "#    x = x.cuda()\n",
    "#    x_test = x_test.cuda()\n",
    "#    yb = yb.cuda()\n",
    "#    ya = ya.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the analytic solution as a function of x\n",
    "y = sol(x)\n",
    "\n",
    "# get the analytic right-hand-side as a function of y(x)\n",
    "# f(x) = dy(x)/dx\n",
    "dydx = rhs(y)\n",
    "\n",
    "# get the analytic solution at the test points x_test\n",
    "y_test = sol(x_test)\n",
    "    \n",
    "# we will want to propagate gradients through y, dydx, and x\n",
    "# so make them PyTorch Variables\n",
    "x = Variable(x, requires_grad=True)\n",
    "y = Variable(y, requires_grad=True)\n",
    "dydx = Variable(dydx, requires_grad=True)\n",
    "\n",
    "# we will need to evaluate gradients w.r.t. x multiple\n",
    "# times so tell PyTorch to save the gradient variable in x.\n",
    "x.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numpy versions of x,y,f on the cpu for plotting\n",
    "xnp = x.cpu().data.numpy()\n",
    "ynp = y.cpu().data.numpy()\n",
    "fnp = dydx.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Models, Optimizers & Loss Function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenNet(nn.Module):\n",
    "    def __init__(self, n_independent, n_dependent,\n",
    "                 n_hidden, hidden_depth, activation):\n",
    "        super(HiddenNet, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.input_layer = nn.Linear(n_independent, n_hidden)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(hidden_depth):\n",
    "            self.hidden_layers.append(nn.Linear(n_hidden, n_hidden))\n",
    "        self.output_layer = nn.Linear(n_hidden, n_dependent)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for h in self.hidden_layers:\n",
    "            x = self.activation(h(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.y0_0 = torch.nn.Parameter(torch.tensor(1.0))\n",
    "        self.y1_0 = torch.nn.Parameter(torch.tensor(1.0))\n",
    "        self.a = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ya & yb are tensors that let us express the analytic\n",
    "        # solution of the system in matrix form\n",
    "        ya = torch.ones(1,2)\n",
    "        ya[0][0] =  self.y0_0\n",
    "        ya[0][1] = -self.y0_0\n",
    "\n",
    "        yb = torch.ones(1,2)\n",
    "        yb[0][0] = 0\n",
    "        yb[0][1] = self.y0_0 + self.y1_0\n",
    "        \n",
    "#        if use_cuda:\n",
    "#            ya = ya.cuda()\n",
    "#            yb = yb.cuda()\n",
    "\n",
    "        return torch.exp(-self.a * x) * ya + yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_independent_inputs = 1\n",
    "n_dependent_outputs = 2\n",
    "n_hidden_layers = 2\n",
    "hidden_layer_depth = 0\n",
    "activation_function = F.celu\n",
    "\n",
    "if use_exact_model:\n",
    "    net = ExactModel()\n",
    "else:\n",
    "    net = HiddenNet(n_independent=n_independent_inputs, n_dependent=n_dependent_outputs,\n",
    "                    n_hidden=n_hidden_layers, hidden_depth=hidden_layer_depth, activation=activation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if use_cuda:\n",
    "#    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiddenNet(\n",
      "  (input_layer): Linear(in_features=1, out_features=2, bias=True)\n",
      "  (hidden_layers): ModuleList()\n",
      "  (output_layer): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer_adam = torch.optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = []\n",
    "# get a multipanel figure showing the prediction (p) and error (e)\n",
    "fig, (axis_p, axis_f, axis_e) = plt.subplots(nrows=3, ncols=1, figsize=(8,8), dpi=150)\n",
    "axis_e1 = axis_e.twinx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays for accumulating the epoch index and losses for plotting\n",
    "epochs = []\n",
    "losses = []\n",
    "losses0 = []\n",
    "losses1 = []\n",
    "tlosses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is the training loop over epochs\n",
    "# where 1 epoch trains over the whole training dataset\n",
    "def train_error(NumEpochs):\n",
    "    for t in range(NumEpochs):\n",
    "        # calculate prediction given the current net state\n",
    "        prediction = net(x)\n",
    "        \n",
    "        # get error with testing samples\n",
    "        prediction_test = net(x_test)\n",
    "        test_loss = torch.sqrt(loss_func(prediction_test, y_test)).cpu().data.numpy()\n",
    "\n",
    "        # calculate error between prediction and analytic truth y\n",
    "        loss0 = torch.sqrt(loss_func(prediction, y))\n",
    "\n",
    "        # calculate gradients d(prediction)/d(x) for each component\n",
    "\n",
    "        # first, zero out the existing gradients to avoid\n",
    "        # accumulating gradients on top of existing gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        if x.grad is not None:\n",
    "            x.grad.data.zero_()\n",
    "\n",
    "        # now get the gradients dp0/dx\n",
    "        prediction[:,0].backward(torch.ones_like(prediction[:,0]), retain_graph=True)\n",
    "        # clone the x gradient to save a copy of it as dp0/dx\n",
    "        dp0dx = x.grad.clone()\n",
    "        # clear the x gradient for the loss gradient below\n",
    "        x.grad.data.zero_()\n",
    "        \n",
    "        # get gradient dp1/dx\n",
    "        prediction[:,1].backward(torch.ones_like(prediction[:,1]), retain_graph=True)\n",
    "        # clone the x gradient to save a copy of it as dp1/dx\n",
    "        dp1dx = x.grad.clone()\n",
    "        # clear the x gradient for the loss gradient below\n",
    "        x.grad.data.zero_()\n",
    "        \n",
    "        dpdx = torch.ones_like(prediction)\n",
    "        dpdx[:,0] = torch.flatten(dp0dx)\n",
    "        dpdx[:,1] = torch.flatten(dp1dx)\n",
    "        \n",
    "        # evaluate the analytic right-hand-side function at the prediction value\n",
    "        prhs = rhs(prediction)\n",
    "\n",
    "        # define the error of the prediction derivative using the analytic derivative\n",
    "        loss1 = torch.sqrt(loss_func(dpdx, dydx))\n",
    "        \n",
    "        # the following doesn't work well :/\n",
    "        #loss1 = torch.sqrt(loss_func(dpdx, rhs(prediction)))\n",
    "\n",
    "        # total error combines the error of the prediction (loss0) with \n",
    "        # the error of the prediction derivative (loss1)\n",
    "        loss = loss0 + loss1\n",
    "\n",
    "        # use the Adam optimizer\n",
    "        optimizer = optimizer_adam\n",
    "\n",
    "        # clear gradients for the next training iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute backpropagation gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # apply gradients to update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # generate plots\n",
    "        if t % 10 == 0:\n",
    "            # Prediction plot to show learning progress\n",
    "            axis_p.clear()\n",
    "\n",
    "            axis_p.set_ylabel('Solution', fontsize=22)\n",
    "\n",
    "            pnp = prediction.cpu().data.numpy()\n",
    "            \n",
    "            axis_p.plot(xnp, pnp[:,0],\n",
    "                        'orange', lw=3, alpha=0.5,\n",
    "                        label='p(t)')\n",
    "            \n",
    "            axis_p.plot(xnp, pnp[:,1],\n",
    "                        'red', lw=3, alpha=0.5)\n",
    "            \n",
    "            axis_p.scatter(xnp, ynp[:,0],\n",
    "                           color='blue', alpha=0.5, s=20,\n",
    "                           label='x(t)')\n",
    "\n",
    "            axis_p.scatter(xnp, ynp[:,1],\n",
    "                           color='green', alpha=0.5, s=20)\n",
    "            \n",
    "            axis_p.legend(loc='upper right', borderpad=1, framealpha=0.5)\n",
    "            \n",
    "            # Plot analytic rhs vs prediction rhs\n",
    "            pfnp = prhs.cpu().data.numpy()\n",
    "            dpdxnp = dpdx.cpu().data.numpy()\n",
    "            axis_f.clear()\n",
    "\n",
    "            axis_f.set_ylabel('Gradient', fontsize=22)\n",
    "            \n",
    "            axis_f.plot(xnp, pfnp[:,0],\n",
    "                        'orange', lw=3, alpha=0.5,\n",
    "                        label='f(p(t))')\n",
    "            \n",
    "            axis_f.plot(xnp, pfnp[:,1],\n",
    "                        'red', lw=3, alpha=0.5)\n",
    "            \n",
    "            axis_f.plot(xnp, dpdxnp[:,0],\n",
    "                        'magenta', lw=3, ls=':', alpha=0.5,\n",
    "                        label='dp(t)/dt')\n",
    "            \n",
    "            axis_f.plot(xnp, dpdxnp[:,1],\n",
    "                        'black', lw=3, ls=':', alpha=0.5)\n",
    "\n",
    "            axis_f.scatter(xnp, fnp[:,0],\n",
    "                           color='blue', alpha=0.5, s=20,\n",
    "                           label='f(x(t))')\n",
    "\n",
    "            axis_f.scatter(xnp, fnp[:,1],\n",
    "                           color='green', alpha=0.5, s=20)\n",
    "            \n",
    "            axis_f.tick_params(axis='both', which='major', labelsize=16)\n",
    "            \n",
    "            axis_f.legend(loc='upper right', borderpad=1, framealpha=0.5)\n",
    "\n",
    "            # get min/max in x/y to set label positions relative to the axes\n",
    "            height = np.abs(ymax - ymin)\n",
    "            width = np.abs(xmax - xmin)\n",
    "\n",
    "            axis_p.set_xlim(xmin, xmax)\n",
    "            axis_p.set_ylim(ymin, ymax)\n",
    "\n",
    "            axis_p.text(xmin, ymax + height*0.3,\n",
    "                      'Step = %d' % t, fontdict={'size': 24, 'color': 'blue'})\n",
    "            axis_p.text(xmin + width*0.5, ymax + height*0.3,\n",
    "                      'Train Loss = %.2e' % loss.cpu().data.numpy(),\n",
    "                      fontdict={'size': 24, 'color': 'blue'})\n",
    "            axis_p.text(xmin + width*0.5, ymax + height*0.1,\n",
    "                      'Test Loss = %.2e' % test_loss,\n",
    "                      fontdict={'size': 24, 'color': 'orange'})\n",
    "\n",
    "            axis_p.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "            # plot errors evolving with the number of epochs trained\n",
    "            epochs.append(t)\n",
    "            losses.append(loss.cpu().data.numpy())\n",
    "            losses0.append(loss0.cpu().data.numpy())\n",
    "            losses1.append(loss1.cpu().data.numpy())\n",
    "            tlosses.append(test_loss)\n",
    "\n",
    "            axis_e.clear()\n",
    "            axis_e1.clear()\n",
    "\n",
    "            axis_e.set_xlabel('Epoch', fontsize=22)\n",
    "            axis_e.set_ylabel('E(p,x)', fontsize=22)\n",
    "\n",
    "            axis_e.scatter([epochs[-1]], [losses0[-1]],\n",
    "                           color=\"red\", alpha=0.5)\n",
    "            axis_e.plot(epochs, losses0,\n",
    "                        'b-', lw=3, alpha=0.5,\n",
    "                        label='E(p,x) [train]')\n",
    "\n",
    "            axis_e.scatter([epochs[-1]], [test_loss],\n",
    "                           color=\"red\", alpha=0.5)\n",
    "            axis_e.plot(epochs, tlosses,\n",
    "                        'orange', lw=3, ls=\"--\", alpha=0.5,\n",
    "                        label='E(p,x) [test]')\n",
    "\n",
    "            axis_e1.set_ylabel('E(dp/dt, f(x))', fontsize=22)\n",
    "\n",
    "            axis_e1.scatter([epochs[-1]], [losses1[-1]],\n",
    "                           color=\"red\", alpha=0.5)\n",
    "            axis_e1.plot(epochs, losses1,\n",
    "                         'g-', lw=3, alpha=0.5,\n",
    "                         label='E(dp/dt, f(x)) [train]')\n",
    "            \n",
    "            axis_e.get_yaxis().set_major_formatter(\n",
    "                matplotlib.ticker.FuncFormatter(lambda x, p: \"{:0.1f}\".format(x)))\n",
    "            \n",
    "            axis_e1.get_yaxis().set_major_formatter(\n",
    "                matplotlib.ticker.FuncFormatter(lambda x, p: \"{:0.1f}\".format(x)))\n",
    "\n",
    "            axis_e.tick_params(axis='both', which='major', labelsize=16)\n",
    "            axis_e1.tick_params(axis='both', which='major', labelsize=16)\n",
    "            \n",
    "            axis_e.legend(loc='upper right', borderpad=1, framealpha=0.5)\n",
    "            axis_e1.legend(loc='upper center', borderpad=1, framealpha=0.5)\n",
    "\n",
    "            # Draw on canvas and save image in sequence\n",
    "            fig.canvas.draw()\n",
    "            plt.tight_layout()\n",
    "            image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "            image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "            training_images.append(image)\n",
    "\n",
    "            # Print epoch/error notifications\n",
    "            if t % 100 == 0:\n",
    "                print('epoch ', t, ' with error: ', losses[-1])\n",
    "           \n",
    "        # Stop early if our errors are plateauing\n",
    "        if t > 1000:\n",
    "            # do a quadratic polynomial fit and see if we will\n",
    "            # need more than NumEpochs for the error e to vanish:\n",
    "            # e / (d(e)/d(epoch)) > NumEpochs ?\n",
    "            # if so, then break out of the training loop ...\n",
    "            xfit = epochs[-4:]\n",
    "            efit = losses[-4:]\n",
    "            coef = np.polyfit(xfit, efit, 2)\n",
    "            \n",
    "            if coef[2]/coef[1] > NumEpochs:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  with error:  2.237783\n",
      "epoch  100  with error:  0.354963\n",
      "epoch  200  with error:  0.31930387\n",
      "epoch  300  with error:  0.28354263\n",
      "epoch  400  with error:  0.24477826\n",
      "epoch  500  with error:  0.1411982\n",
      "epoch  600  with error:  0.044161655\n",
      "epoch  700  with error:  0.03513615\n",
      "epoch  800  with error:  0.028354764\n",
      "epoch  900  with error:  0.024161939\n"
     ]
    }
   ],
   "source": [
    "train_error(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('./beta_decay.gif', training_images, duration=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final test sample error: \", tlosses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model File We Trained\n",
    "\n",
    "This uses the just-in-time compiler to compile the network into TorchScript (which is portable across python/C++) and then saves that script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(net)\n",
    "\n",
    "model_scripted.save('beta_decay_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model We Trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our just-in-time compiled TorchScript model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model = torch.jit.load('beta_decay_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our loaded model and compare it to the model we originally saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_saved_model = saved_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_model = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff = eval_saved_model - original_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if all went well, the difference between the saved model and the model we finished training should be 0s everywhere when we evaluate it on the training data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
