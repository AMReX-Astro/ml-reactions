{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maestroflame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the path to your data files\n",
    "data_path = '../../data/data1/flame/'\n",
    "\n",
    "#These is the input/output prefix of your datafile names.\n",
    "input_prefix = 'react_inputs_*'\n",
    "output_prefix = 'react_outputs_*'\n",
    "\n",
    "#Plotfile prefixes, used for visualization purposes.\n",
    "plotfile_prefix = 'flame_*'\n",
    "\n",
    "#By default, this package will save your model, logs of the training and testing data during training,\n",
    "#and plots to a directory. Here you specify that directory.\n",
    "output_dir = 'testing123/'\n",
    "\n",
    "#The log file. Everything that is printed during training also goes into this file in case something\n",
    "#gets interrupted.\n",
    "log_file = output_dir + \"log.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We first remove the directory we just generated. If you don't do this you'll get an error. This is to protect\n",
    "#this package from overwriting your data in case one forgets to change the output_dir when training a new model\n",
    "\n",
    "!rm -r testing123/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maestroflame.train import NuclearReactionML\n",
    "nrml = NuclearReactionML(data_path, input_prefix, output_prefix, plotfile_prefix,\n",
    "                output_dir, log_file, DEBUG_MODE=True, DO_PLOTTING=True,\n",
    "                SAVE_MODEL=True, DO_HYPER_OPTIMIZATION=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the package provides preset up loss functions and networks to use\n",
    "from maestroflame.losses import log_loss\n",
    "from maestroflame.networks import Net\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 2\n",
    "model = Net(16, 16, 16, 16, 14)\n",
    "\n",
    "# get model to cuda if possible\n",
    "model.to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "nrml.train(model, optimizer, num_epochs, log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to put model on cpu for plotting\n",
    "model.to(device=torch.device(\"cpu\"))\n",
    "\n",
    "nrml.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "plots = glob.glob('testing123/*.png')\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for plot in plots:\n",
    "    fig = Image(filename=(plot))\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The pinn class has the exact same interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/data3/flame/'\n",
    "input_prefix = 'react_inputs_*'\n",
    "output_prefix = 'react_outputs_*'\n",
    "plotfile_prefix = 'flame_*'\n",
    "output_dir = 'testing123/'\n",
    "log_file = output_dir + \"log.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We first remove the directory we just generated. If you don't do this you'll get an error. This is to protect\n",
    "#this package from overwriting your data in case one forgets to change the output_dir when training a new model\n",
    "!rm -r testing123/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maestroflame.train import NuclearReactionPinn\n",
    "nrml = NuclearReactionPinn(data_path, input_prefix, output_prefix, plotfile_prefix,\n",
    "                output_dir, log_file, DEBUG_MODE=True, DO_PLOTTING=True,\n",
    "                SAVE_MODEL=True, DO_HYPER_OPTIMIZATION=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A much more complicated loss function\n",
    "\n",
    "import torch.nn as nn\n",
    "from maestroflame.losses import component_loss_f, loss_pinn, rms_weighted_error\n",
    "from maestroflame.losses import log_loss, loss_mass_fraction, component_loss_f_L1, relative_loss\n",
    "from maestroflame.losses import derivative_loss_piecewise, signed_loss_function\n",
    "\n",
    "nnuc=13\n",
    "\n",
    "def criterion(data, pred, dXdt, actual):\n",
    "    #I still don't understand this one but i think don does\n",
    "    #loss1 = rms_weighted_error(pred, actual[:, :nnuc+1], actual[:, :nnuc+1])\n",
    "\n",
    "    # #difference in state variables vs prediction.\n",
    "    # loss1 = log_loss(pred, actual[:, :nnuc+1])\n",
    "    # #physics informed loss\n",
    "    # loss2 = loss_pinn(data, pred, actual, enuc_fac, enuc_dot_fac, log_option=True)\n",
    "    # #sum of mass fractions must be 1\n",
    "    # loss3 = loss_mass_fraction(pred)\n",
    "    # #relative loss function.\n",
    "    # loss4 = relative_loss(pred, actual[:, :nnuc+1])\n",
    "\n",
    "\n",
    "    #difference in state variables vs prediction.\n",
    "    loss1 = log_loss(pred, actual[:, :nnuc+1])\n",
    "    #scaled rates (pinn part) This only scales the magnitude of rates\n",
    "    #loss2 = derivative_loss_piecewise(dXdt, actual[:, nnuc+1:], enuc_fac, enuc_dot_fac)\n",
    "    L = nn.L1Loss()\n",
    "    loss2 = L(dXdt, actual[:, nnuc+1:])\n",
    "\n",
    "    #here we learn the sign of rates to make up for not doing that in loss2\n",
    "    loss3 = signed_loss_function(dXdt, actual[:, nnuc+1:])\n",
    "    #relative loss function. Helps disginguish between same errors of different\n",
    "    #scales since we're scaling the loss2 so heavily\n",
    "    loss4 = relative_loss(pred, actual[:, :nnuc+1])\n",
    "    #sum of mass fractions must be 1\n",
    "    loss5 = loss_mass_fraction(pred)\n",
    "    #sum of rates must be 0\n",
    "    #loss6 = loss_rates_mass_frac(dXdt, actual[:, nnuc+1:])\n",
    "\n",
    "    # loss_arr = [loss1.item(), loss2.item(), loss3.item(), loss4.item(), loss5.item()]\n",
    "    # return  loss1 + loss2 + loss3  + loss4 + loss5, loss_arr\n",
    "    loss_arr = [loss1.item(), loss2.item(), loss3.item(), loss5.item()]\n",
    "    return  loss1 + loss2 + loss3 + loss5, loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maestroflame.networks import Net\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 2\n",
    "model = Net(16, 16, 16, 16, 14)\n",
    "\n",
    "# get model to cuda if possible\n",
    "model.to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "nrml.train(model,optimizer, num_epochs, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to put model on cpu for plotting\n",
    "model.to(device=torch.device(\"cpu\"))\n",
    "\n",
    "nrml.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "plots = glob.glob('testing123/*.png')\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for plot in plots:\n",
    "    fig = Image(filename=(plot))\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
